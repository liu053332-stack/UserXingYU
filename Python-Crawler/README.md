# Python-Crawler

## 电影天堂资源爬虫

一个优化后的Python多线程网络爬虫，用于自动爬取电影天堂网站上的电影资源下载链接。

### 主要功能
- 自动爬取电影天堂全部分类的电影资源
- 多线程并行爬取，提高效率
- 自动创建分类目录并保存下载链接
- 完善的错误处理和日志记录
- 支持爬取速率控制，避免对目标网站造成过大压力

### 技术栈
- Python 3.x
- 第三方库：`requests`, `lxml`

### 优化亮点

1. **Python 3兼容** - 完全支持现代Python版本
2. **面向对象设计** - 代码结构清晰，易于维护和扩展
3. **线程池并发** - 使用`concurrent.futures`实现高效的线程管理
4. **健壮的异常处理** - 网络请求重试机制和全面的错误捕获
5. **完整的日志系统** - 同时输出到控制台和日志文件
6. **灵活的配置选项** - 可自定义存储路径、线程数、爬取间隔等
7. **智能URL去重** - 使用集合数据结构提高去重效率
8. **目录自动创建** - 包括备选路径方案，避免因路径问题导致程序失败

### 安装说明

1. 确保已安装Python 3.x
2. 安装依赖库：
   ```bash
   pip install -r requirements.txt
   ```

### 使用方法

1. 直接运行主程序：
   ```bash
   python Crawl.py
   ```

2. 默认配置下，程序会：
   - 从电影天堂首页开始爬取
   - 在`D:/电影资源/`目录下创建分类文件夹
   - 将每个电影的下载链接保存到对应的文本文件中

### 配置选项

您可以在代码开头的配置部分修改以下参数：

- `STORAGE_PATH` - 下载链接的保存路径
- `MAX_WORKERS` - 最大并发线程数
- `CRAWL_INTERVAL` - 爬取间隔时间（秒）
- `TIMEOUT` - 网络请求超时时间（秒）

### 注意事项

1. 请确保您有足够的磁盘空间来存储下载链接文件
2. 过度爬取可能会导致IP被目标网站暂时封禁，建议保持合理的爬取间隔
3. 该工具仅用于学习和研究目的，请尊重网站的robots.txt规则
4. 如果D盘不存在，程序会自动尝试在当前目录创建"电影资源"文件夹

### 日志信息

程序运行时会同时在控制台和`crawler.log`文件中输出日志信息，包括：
- 爬取进度和状态
- 错误和异常信息
- 创建的文件和目录信息

### 可能的问题和解决方案

1. **无法创建目录**：检查目录权限或修改`STORAGE_PATH`配置
2. **爬取失败**：可能是网站结构变化或网络问题，可以查看日志获取详细信息
3. **IP被封禁**：降低爬取频率，增加`CRAWL_INTERVAL`值

### 更新日志

- 优化版本：完全重构代码，升级到Python 3，添加线程池、完善错误处理和日志系统
